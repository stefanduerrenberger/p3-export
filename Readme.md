# P3 Crawler

## Overview

This was used to migrate content from the Swiss Planet3 Greenpeace.org website into a wordpress installation. It uses [Scrapy](https://scrapy.org/) to extract content from greenpeace.org (Planet3 version). There's a crawler defined to get blogs, publications and press releases, tailored to the Swiss version of the website in German and French. Results are saved as XML or JSON data.

The resulting XML file can be easily imported into Wordpress using WP All Import. Image downloads were handled by WP All Import with an additional script that converts the file names during import.

Sorry for the bad coding style. I'm sure many things could be done better. This was my first try with Python and it had to be done quickly. Feel free to suggest changes.

## Warning

Running this puts heavy loads on the webserver. Please be considerate. When testing, please set a low depth limit instead of getting all content.

## Installation

- Install [Scrapy](https://scrapy.org/)
- Donwload the project
- Change the URLs and settings (output files, limit) in the project (this was made for one time use only, so URLs are hard coded in the spiders)
- Make sure to also change the locale in the spider files and the pipeline. It's used to convert the date in articles.
- You my also have to change the date formats used in the pipeline file to the ones you use on your website. 
- Depending on how your P3 webiste is used, you may have to adapt some of the definitions of where to get the content from
- Note that the pipelina also converts categories and tags into new tags. We drastically reduced the tags we wanted to use on the new website. All the conversions are saved to a file called category-conversion.csv

## Files explanation

- category-conversion.csv: Contains a list of conversions to be done on tags and categories. Used to clean up tagging in the content. Empty the list or remove that part from gpposts/pipelines.py if you don't want to use tag conversion.
- example_output.xml: Example XML output from running the blog spider.
- gpposts/pipelines.py: Pipelines used for cleaning up the data after it has been scraped.
- gpposts/settings.py: Most settings are here, some you can find directly in the spider files.
- gpposts/spiders/blogs_spider.py: Spider for blog posts. Adapt to your needs.
- gpposts/spiders/media_spider.py: Spider for Press Releasses . Adapt to your needs.
- pages-to-archive.csv: A list of all the pages that the pages spider should crawl
- gpposts/spiders/pages_spider.py: Spider for pages. Different from the other spiders that find articles, this one reads from a list (pages-to-archive.csv) of pages and gets content of those. We manually put this list together because we didn't want to import too much of the old content. Adapt to your needs.
- gpposts/spiders/publications_spider.py: Spider for publications (reports) posts. Adapt to your needs.
- redirects/generate-redirects.php: PHP script to generate redirects from old URLs to new ones. Used as standalone and works by getting the info from the Wordpress database after the import. (And no, that's not our production DB password. Don't even try.)

## Extract the content

- Run the scrapy spiders from your terminal (example: scrapy crawl blogs)

## Import into Wordpress

- Install WP all Import
- In the settings, import the profiles
- Use the XML files generated by scrapy in WP All Import. The functions to rename the image paths is NOT imported with the rest of the import profiles. You'll have to manually add and save them in Step 3 of the import process.
- If you have search or image optimizing plugins, turn them off for the import, it takes long enough without them.
- Start the import and wait. 

